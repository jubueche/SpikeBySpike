\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\providecommand \oddpage@label [2]{}
\citation{Maass:2002:RCW:639717.639718}
\citation{wiel2017learning}
\citation{Vincent:2008:ECR:1390156.1390294}
\citation{wiel2017learning}
\citation{Lapicque}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory: Representing signals spike-by-spike}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Network dynamics}{2}{subsection.2.1}\protected@file@percent }
\newlabel{voltage_dynamic_diff}{{1}{2}{Network dynamics}{equation.2.1}{}}
\citation{Dale1935}
\citation{wiel2017learning}
\citation{Tibshirani1996}
\newlabel{voltage_dynamic_explicit}{{2}{3}{Network dynamics}{equation.2.2}{}}
\newlabel{rate_diff}{{3}{3}{Network dynamics}{equation.2.3}{}}
\newlabel{input_diff}{{4}{3}{Network dynamics}{equation.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Input $x$ of dimension $N_x = 2$ shown over a period of 1000 ms.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:input}{{1}{3}{Input $x$ of dimension $N_x = 2$ shown over a period of 1000 ms}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The optimal decoder}{3}{subsection.2.2}\protected@file@percent }
\newlabel{loss_function_optimal_decoder}{{5}{3}{The optimal decoder}{equation.2.5}{}}
\citation{wiel2017learning}
\citation{Bourdoukan:2012:LOS:2999325.2999390}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Optimal network connectivity}{4}{subsection.2.3}\protected@file@percent }
\newlabel{loss_smaller}{{6}{4}{Optimal network connectivity}{equation.2.6}{}}
\newlabel{rate_plus_unit_l2}{{7}{4}{Optimal network connectivity}{equation.2.7}{}}
\newlabel{rate_plus_unit_l1}{{8}{4}{Optimal network connectivity}{equation.2.8}{}}
\newlabel{network_dynamics}{{9}{4}{Optimal network connectivity}{equation.2.9}{}}
\newlabel{voltage_via_dynamics}{{10}{4}{Optimal network connectivity}{equation.2.10}{}}
\newlabel{threshold_via_dynamics}{{11}{4}{Optimal network connectivity}{equation.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Learning optimal recurrent connectivity}{5}{subsection.2.4}\protected@file@percent }
\newlabel{balanced_voltage}{{12}{5}{Learning optimal recurrent connectivity}{equation.2.12}{}}
\newlabel{loss_voltage_before_after}{{13}{5}{Learning optimal recurrent connectivity}{equation.2.13}{}}
\newlabel{relation_voltage_before_after}{{14}{5}{Learning optimal recurrent connectivity}{equation.2.14}{}}
\citation{DBLP:journals/corr/abs-1708-04198}
\citation{wiel2017learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Simulation and results}{6}{subsection.2.5}\protected@file@percent }
\newlabel{algorithm_simulation}{{1}{6}{Simulation and results}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Algorithm describing the learning procedure without learning the feed-forward weights. $\Omega _k$ and $I_k$ reference the $k$-th column of the respective matrix and $\ast $ is the convolution operator.}}{6}{algocf.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (i) Convergence of optimal weights (ii) Convergence of error (iii) Convergence of mean firing rate (iv) Convergence of voltage variance}}{7}{figure.2}\protected@file@percent }
\newlabel{fig:convergence}{{2}{7}{(i) Convergence of optimal weights (ii) Convergence of error (iii) Convergence of mean firing rate (iv) Convergence of voltage variance}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The two-dimensional input is reconstructed pre-learning (row 1 \& 2) and post-learning (row 4 \& 5). Due to the evolving tight balance, the population spike trains sparsify (row 3 \& 6).}}{8}{figure.3}\protected@file@percent }
\newlabel{fig:reconstruction}{{3}{8}{The two-dimensional input is reconstructed pre-learning (row 1 \& 2) and post-learning (row 4 \& 5). Due to the evolving tight balance, the population spike trains sparsify (row 3 \& 6)}{figure.3}{}}
\citation{Boybat2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Limitations of the theory}{9}{subsection.2.6}\protected@file@percent }
\newlabel{sec:limitations}{{2.6}{9}{Limitations of the theory}{subsection.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The spike train after training using continous weights (top), exhibits a usable, distributed pattern, while the learned, discretized weight matrix (bottom) shows a subset of neurons firing at a high frequency.}}{9}{figure.4}\protected@file@percent }
\newlabel{fig:discrete_spike_trains}{{4}{9}{The spike train after training using continous weights (top), exhibits a usable, distributed pattern, while the learned, discretized weight matrix (bottom) shows a subset of neurons firing at a high frequency}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Convergence of different properties when learning with discrete weights (orange) and performing the updates with respect to all spiking neurons (green). It should be noted that the mean firing rate is really high compared to the successful learning (see Figure \ref  {fig:convergence}).}}{10}{figure.5}\protected@file@percent }
\newlabel{fig:convergence_ua_discrete}{{5}{10}{Convergence of different properties when learning with discrete weights (orange) and performing the updates with respect to all spiking neurons (green). It should be noted that the mean firing rate is really high compared to the successful learning (see Figure \ref {fig:convergence})}{figure.5}{}}
\citation{DBLP:journals/corr/abs-1708-04198}
\citation{Deiss:1999:PCI:296533.296540}
\citation{Cassidy2016TrueNorthAH}
\citation{Cassidy2016TrueNorthAH}
\@writefile{toc}{\contentsline {section}{\numberline {3}Hardware: The DYNAP-SE chip}{11}{section.3}\protected@file@percent }
\newlabel{sec:DYNAP-SE}{{3}{11}{Limitations of the theory}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces (l) Each neuron can utilize at most 64 synapses, meaning that it can have at most 64 unique connections. This imposes a severe constraint on network precision, since weights are usually a multiple of single synapses, as described in section (\ref  {sec:weights}). (r) Each neuron is assigned an address that is encoded as a digital word and transmitted as soon as the neuron spikes/triggers an event. Using the transmitted address and arrival time, spikes can be decoded again.}}{11}{figure.6}\protected@file@percent }
\newlabel{fig:routing}{{6}{11}{(l) Each neuron can utilize at most 64 synapses, meaning that it can have at most 64 unique connections. This imposes a severe constraint on network precision, since weights are usually a multiple of single synapses, as described in section (\ref {sec:weights}). (r) Each neuron is assigned an address that is encoded as a digital word and transmitted as soon as the neuron spikes/triggers an event. Using the transmitted address and arrival time, spikes can be decoded again}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The DYNAP-SE has a core area of $0.36mm^2$ and utilizes roughly $1.68nJ$ per spike. In comparison: IBM's True North (\cite  {Cassidy2016TrueNorthAH}) utilizes $3.9nJ$ on $0.094mm^2$ core area.}}{11}{figure.7}\protected@file@percent }
\newlabel{fig:routing}{{7}{11}{The DYNAP-SE has a core area of $0.36mm^2$ and utilizes roughly $1.68nJ$ per spike. In comparison: IBM's True North (\cite {Cassidy2016TrueNorthAH}) utilizes $3.9nJ$ on $0.094mm^2$ core area}{figure.7}{}}
\citation{DBLP:journals/corr/abs-1708-04198}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Following a tree-based routing strategy, groups of four cores can communicate with other cores in the same group using level 1 R2 routers or between different groups on the same tile using level 2 R2 routers. To reach another tile, the level 3 R2 router is used. Following a 2D-mesh approach, the R3 router can be used to communicate between multiple chips.}}{12}{figure.8}\protected@file@percent }
\newlabel{fig:routing}{{8}{12}{Following a tree-based routing strategy, groups of four cores can communicate with other cores in the same group using level 1 R2 routers or between different groups on the same tile using level 2 R2 routers. To reach another tile, the level 3 R2 router is used. Following a 2D-mesh approach, the R3 router can be used to communicate between multiple chips}{figure.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Learning optimal spike-based signal representations on the DYNAP-SE}{12}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Setup: Learning in-the-loop}{12}{subsection.4.1}\protected@file@percent }
\citation{corradi}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The feed-forward connections $\mathbf  {F^TM}$ (derived in section \ref  {sec:spiking-input}) are initially discretized to $\mathbb  {Z}$ and loaded onto the chip. The following four steps are executed repeatedly for a fixed number of iterations: A new signal is generated and stored in the FPGA memory, the recurrent weights $\mathbf  {\Omega }$ are updated, discretized to $\mathbb  {Z}$ and loaded onto the chip, the DYNAP-SE runs on the spiking input and finally $\Delta \mathbf  {\Omega }$ is computed using the spike trains $\mathbf  {O}^{\textnormal  {DYNAP-SE}}$ recorded from the DYNAP-SE.}}{13}{figure.9}\protected@file@percent }
\newlabel{fig:in-the-loop}{{9}{13}{The feed-forward connections $\mathbf {F^TM}$ (derived in section \ref {sec:spiking-input}) are initially discretized to $\mathbb {Z}$ and loaded onto the chip. The following four steps are executed repeatedly for a fixed number of iterations: A new signal is generated and stored in the FPGA memory, the recurrent weights $\mathbf {\Omega }$ are updated, discretized to $\mathbb {Z}$ and loaded onto the chip, the DYNAP-SE runs on the spiking input and finally $\Delta \mathbf {\Omega }$ is computed using the spike trains $\mathbf {O}^{\textnormal {DYNAP-SE}}$ recorded from the DYNAP-SE}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Aligning on- and off chip network dynamics}{13}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Network with spiking input}{13}{subsubsection.4.2.1}\protected@file@percent }
\newlabel{sec:spiking-input}{{4.2.1}{13}{Network with spiking input}{subsubsection.4.2.1}{}}
\newlabel{recon_input}{{15}{13}{Network with spiking input}{equation.4.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The signal (top) is split into two channels, representing the (upward) downward magnitude of the signal in the number of spikes.}}{14}{figure.10}\protected@file@percent }
\newlabel{fig:delta_modulated_input}{{10}{14}{The signal (top) is split into two channels, representing the (upward) downward magnitude of the signal in the number of spikes}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces While the error convergence of the spiking-input and continous-input network matches, the error of the spiking-input network is increased by roughly a factor of 10, due to the loss of precision imposed by the spikes (note the green scale).}}{15}{figure.11}\protected@file@percent }
\newlabel{fig:spiking_vs_continous}{{11}{15}{While the error convergence of the spiking-input and continous-input network matches, the error of the spiking-input network is increased by roughly a factor of 10, due to the loss of precision imposed by the spikes (note the green scale)}{figure.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}The weights on-chip}{16}{subsubsection.4.2.2}\protected@file@percent }
\newlabel{sec:weights}{{4.2.2}{16}{The weights on-chip}{subsubsection.4.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Due to discretization of the matrix, precision is lost. Note that in the right plot, only 5 distinct discrete values are visible, while the right matrix is continous. However, the global structure is preserved and the matrix is symmetric, as the graph should be undirected.}}{16}{figure.12}\protected@file@percent }
\newlabel{fig:spiking_vs_continous}{{12}{16}{Due to discretization of the matrix, precision is lost. Note that in the right plot, only 5 distinct discrete values are visible, while the right matrix is continous. However, the global structure is preserved and the matrix is symmetric, as the graph should be undirected}{figure.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Batched updates}{17}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Further alignment using time-window update}{17}{subsubsection.4.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The convergence properties using the batched and online versions of the learning rule. The unscaled batched version performs equally well as the online version.}}{18}{figure.13}\protected@file@percent }
\newlabel{fig:batched_update}{{13}{18}{The convergence properties using the batched and online versions of the learning rule. The unscaled batched version performs equally well as the online version}{figure.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}Final pseudo code}{20}{subsubsection.4.2.5}\protected@file@percent }
\newlabel{sec:pseudo-code}{{4.2.5}{20}{Final pseudo code}{subsubsection.4.2.5}{}}
\newlabel{algorithm_chip_in_the_loop}{{2}{20}{Final pseudo code}{algocfline.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces The final pseudo code illustrating the changes made, compared to algorithm \ref  {algorithm_simulation}. Note for example the batched updates or the discretized feed-forward matrix in the beginning. All hyperparameters used can be found in the Appendix. }}{20}{algocf.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Results}{21}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces While reducing the reconstruction error, the spike train sparsifies. In fact, for this experiment 880 spikes were saved over a period of 1 second, which is more than 3 million in one hour. A neuron on the DYNAP-SE consumes roughly 1.68$nJ$, so 5.04$mJ$ were saved for only 20 neurons.}}{21}{figure.14}\protected@file@percent }
\newlabel{fig:DYNAPS_reconstruction}{{14}{21}{While reducing the reconstruction error, the spike train sparsifies. In fact, for this experiment 880 spikes were saved over a period of 1 second, which is more than 3 million in one hour. A neuron on the DYNAP-SE consumes roughly 1.68$nJ$, so 5.04$mJ$ were saved for only 20 neurons}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Besides the decoding error, all variables show stable convergence. Despite the fact that overall decoding error is reduced, learning is still unstable and has to be improved using an adaptive learning rate (see section \ref  {sec:rl}).}}{22}{figure.15}\protected@file@percent }
\newlabel{fig:DYNAPS_convergence}{{15}{22}{Besides the decoding error, all variables show stable convergence. Despite the fact that overall decoding error is reduced, learning is still unstable and has to be improved using an adaptive learning rate (see section \ref {sec:rl})}{figure.15}{}}
\citation{xu2019learning}
\citation{Sutton1988}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Reinforcement Learning based step size adaptation}{23}{subsection.4.4}\protected@file@percent }
\newlabel{sec:rl}{{4.4}{23}{Reinforcement Learning based step size adaptation}{subsection.4.4}{}}
\newlabel{eq:reward}{{4.4}{23}{Reinforcement Learning based step size adaptation}{subsection.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces When discretizing the weights, convergence becomes unstable, which can be countered by an adaptive learning rate. While being biologically plausible, the \textbf  {RL} scheme provides a flexible learning rate, ensuring more stable convergence. One can see that convergence is more stable after approximately 300 iterations.}}{23}{figure.16}\protected@file@percent }
\newlabel{fig:rewards}{{16}{23}{When discretizing the weights, convergence becomes unstable, which can be countered by an adaptive learning rate. While being biologically plausible, the \textbf {RL} scheme provides a flexible learning rate, ensuring more stable convergence. One can see that convergence is more stable after approximately 300 iterations}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Convergence using the \textbf  {RL}-based scheme becomes more stable (cf. Figure \ref  {fig:DYNAPS_convergence}).}}{24}{figure.17}\protected@file@percent }
\newlabel{fig:stable}{{17}{24}{Convergence using the \textbf {RL}-based scheme becomes more stable (cf. Figure \ref {fig:DYNAPS_convergence})}{figure.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Reconstructed signal using a time-window of $\pm 10ms$ and \textbf  {RL}-based lernig rate adaptation with $\beta =10$ and $n=5$. Compared to Figure \ref  {fig:DYNAPS_reconstruction}, one can see that the reconstruction is indeed more precise.}}{25}{figure.18}\protected@file@percent }
\newlabel{fig:stable_reconstruction}{{18}{25}{Reconstructed signal using a time-window of $\pm 10ms$ and \textbf {RL}-based lernig rate adaptation with $\beta =10$ and $n=5$. Compared to Figure \ref {fig:DYNAPS_reconstruction}, one can see that the reconstruction is indeed more precise}{figure.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{25}{section.5}\protected@file@percent }
\bibdata{bibliography}
\bibcite{Bourdoukan:2012:LOS:2999325.2999390}{{1}{2012}{{Bourdoukan et~al.}}{{Bourdoukan, Barrett, Machens, and Den\`{e}ve}}}
\bibcite{Boybat2018}{{2}{2018}{{Boybat et~al.}}{{Boybat, Le~Gallo, Nandakumar, Moraitis, Parnell, Tuma, Rajendran, Leblebici, Sebastian, and Eleftheriou}}}
\bibcite{wiel2017learning}{{3}{2017}{{Brendel et~al.}}{{Brendel, Bourdoukan, Vertechi, Machens, and Den√©ve}}}
\bibcite{Cassidy2016TrueNorthAH}{{4}{2016}{{Cassidy et~al.}}{{Cassidy, Sawada, Merolla, Arthur, Alvarez-Icaza, Akopyan, Jackson, and Modha}}}
\bibcite{corradi}{{5}{2015}{{Corradi and Indiveri}}{{}}}
\bibcite{Dale1935}{{6}{1935}{{Dale}}{{}}}
\bibcite{Deiss:1999:PCI:296533.296540}{{7}{1999}{{Deiss et~al.}}{{Deiss, Douglas, and Whatley}}}
\bibcite{Lapicque}{{8}{1907}{{Lapicque}}{{}}}
\bibcite{Maass:2002:RCW:639717.639718}{{9}{2002}{{Maass et~al.}}{{Maass, Natschlager, and Markram}}}
\bibcite{DBLP:journals/corr/abs-1708-04198}{{10}{2017}{{Moradi et~al.}}{{Moradi, Ning, Stefanini, and Indiveri}}}
\bibcite{Sutton1988}{{11}{1988}{{Sutton}}{{}}}
\bibcite{Tibshirani1996}{{12}{1996}{{Tibshirani}}{{}}}
\bibcite{Vincent:2008:ECR:1390156.1390294}{{13}{2008}{{Vincent et~al.}}{{Vincent, Larochelle, Bengio, and Manzagol}}}
\bibcite{xu2019learning}{{14}{2019}{{Xu et~al.}}{{Xu, Dai, Kemp, and Metz}}}
