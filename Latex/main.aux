\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\providecommand \oddpage@label [2]{}
\citation{chow:68}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory: Representing signals spike-by-spike}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Introduction}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Network dynamics}{2}{subsection.2.2}\protected@file@percent }
\newlabel{voltage_dynamic_diff}{{1}{2}{Network dynamics}{equation.2.1}{}}
\newlabel{voltage_dynamic_explicit}{{2}{2}{Network dynamics}{equation.2.2}{}}
\newlabel{rate_diff}{{3}{3}{Network dynamics}{equation.2.3}{}}
\newlabel{input_diff}{{4}{3}{Network dynamics}{equation.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Input $x$ of dimension $N_x = 2$ shown over a period of 1000 ms.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:input}{{1}{3}{Input $x$ of dimension $N_x = 2$ shown over a period of 1000 ms}{figure.1}{}}
\newlabel{reconstructed_x_explicit}{{5}{3}{Network dynamics}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}The optimal decoder}{3}{subsection.2.3}\protected@file@percent }
\newlabel{loss_function_optimal_decoder}{{6}{3}{The optimal decoder}{equation.2.6}{}}
\newlabel{optimal_decoder_no_lagrangian}{{7}{3}{The optimal decoder}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Optimal network connectivity}{4}{subsection.2.4}\protected@file@percent }
\newlabel{loss_smaller}{{8}{4}{Optimal network connectivity}{equation.2.8}{}}
\newlabel{neuron_n_spiked_loss}{{9}{4}{Optimal network connectivity}{equation.2.9}{}}
\newlabel{rate_plus_unit_l2}{{10}{4}{Optimal network connectivity}{equation.2.10}{}}
\newlabel{rate_plus_unit_l1}{{11}{4}{Optimal network connectivity}{equation.2.11}{}}
\newlabel{network_dynamics}{{12}{4}{Optimal network connectivity}{equation.2.12}{}}
\newlabel{voltage_via_dynamics}{{13}{4}{Optimal network connectivity}{equation.2.13}{}}
\newlabel{threshold_via_dynamics}{{14}{4}{Optimal network connectivity}{equation.2.14}{}}
\newlabel{voltage_diff_via_dynamics}{{15}{4}{Optimal network connectivity}{equation.2.15}{}}
\newlabel{threshold_rewritten}{{16}{5}{Optimal network connectivity}{equation.2.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Learning optimal recurrent connectivity}{5}{subsection.2.5}\protected@file@percent }
\newlabel{balanced_voltage}{{17}{5}{Learning optimal recurrent connectivity}{equation.2.17}{}}
\newlabel{loss_voltage_before_after}{{18}{5}{Learning optimal recurrent connectivity}{equation.2.18}{}}
\newlabel{relation_voltage_before_after}{{19}{5}{Learning optimal recurrent connectivity}{equation.2.19}{}}
\newlabel{updated_recurrent_learning_rule}{{21}{6}{Learning optimal recurrent connectivity}{equation.2.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Simulation and results}{6}{subsection.2.6}\protected@file@percent }
\newlabel{algorithm_simulation}{{1}{6}{Simulation and results}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Algorithm describing the learning procedure without learning the feed-forward weights. The used prameters can be found in the Appendix. $\Omega _k$ and $I_k$ reference the $k$-th column of the respective matrix and $\ast $ is the convolution operator.}}{6}{algocf.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Multiple figures in here: (i) Convergence of optimal weights (ii) Convergence of membrane variance (iii) Convergence of error (iv) Convergence of firing rate}}{7}{figure.2}\protected@file@percent }
\newlabel{fig:convergence}{{2}{7}{Multiple figures in here: (i) Convergence of optimal weights (ii) Convergence of membrane variance (iii) Convergence of error (iv) Convergence of firing rate}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Multiple figures in here: (i) Convergence of optimal weights (ii) Convergence of membrane variance (iii) Convergence of error (iv) Convergence of firing rate}}{8}{figure.3}\protected@file@percent }
\newlabel{fig:reconstruction}{{3}{8}{Multiple figures in here: (i) Convergence of optimal weights (ii) Convergence of membrane variance (iii) Convergence of error (iv) Convergence of firing rate}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Limitations of the theory}{9}{subsection.2.7}\protected@file@percent }
\newlabel{sec:limitations}{{2.7}{9}{Limitations of the theory}{subsection.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The spike train after training using continous weights (top), exhibits a usable, distributed pattern, while the learned, discretized weight matrix (bottom) shows a subset of neurons firing at a high frequency.}}{9}{figure.4}\protected@file@percent }
\newlabel{fig:discrete_spike_trains}{{4}{9}{The spike train after training using continous weights (top), exhibits a usable, distributed pattern, while the learned, discretized weight matrix (bottom) shows a subset of neurons firing at a high frequency}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Convergence of different properties when learning with discrete weights (orange) and performing the updates with respect to all spiking neurons (green). It should be noted that the mean firing rate is really high compared to the successful learning (see Figure \ref  {fig:convergence}).}}{10}{figure.5}\protected@file@percent }
\newlabel{fig:convergence_ua_discrete}{{5}{10}{Convergence of different properties when learning with discrete weights (orange) and performing the updates with respect to all spiking neurons (green). It should be noted that the mean firing rate is really high compared to the successful learning (see Figure \ref {fig:convergence})}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Hardware: The DYNAPS chip}{11}{section.3}\protected@file@percent }
\newlabel{sec:DYNAPS}{{3}{11}{Limitations of the theory}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Learning optimal spike-based signal representations on the DYNAPS}{11}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Setup: Learning in-the-loop}{11}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The feed-forward connections $\mathbf  {F^TM}$ (derived in section \ref  {sec:spiking-input}) are initially discretized to $\mathbb  {Z}$ and loaded onto the chip. The following four steps are executed repeatedly for a fixed number of iterations: A new signal is generated and stored in the FPGA memory, the recurrent weights $\mathbf  {\Omega }$ are updated, discretized to $\mathbb  {Z}$ and loaded onto the chip, the DYNAPS runs on the spiking input and finally $\Delta \mathbf  {\Omega }$ is computed using the spike trains $\mathbf  {O}^{\textnormal  {DYNAPS}}$ recorded from the DYNAPS.}}{11}{figure.6}\protected@file@percent }
\newlabel{fig:in-the-loop}{{6}{11}{The feed-forward connections $\mathbf {F^TM}$ (derived in section \ref {sec:spiking-input}) are initially discretized to $\mathbb {Z}$ and loaded onto the chip. The following four steps are executed repeatedly for a fixed number of iterations: A new signal is generated and stored in the FPGA memory, the recurrent weights $\mathbf {\Omega }$ are updated, discretized to $\mathbb {Z}$ and loaded onto the chip, the DYNAPS runs on the spiking input and finally $\Delta \mathbf {\Omega }$ is computed using the spike trains $\mathbf {O}^{\textnormal {DYNAPS}}$ recorded from the DYNAPS}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Aligning on- and off chip network dynamics}{12}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Network with spiking input}{12}{subsubsection.4.2.1}\protected@file@percent }
\newlabel{sec:spiking-input}{{4.2.1}{12}{Network with spiking input}{subsubsection.4.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The signal (top) is split into two channels, representing the (upward) downward magnitude of the signal in the number of spikes.}}{12}{figure.7}\protected@file@percent }
\newlabel{fig:delta_modulated_input}{{7}{12}{The signal (top) is split into two channels, representing the (upward) downward magnitude of the signal in the number of spikes}{figure.7}{}}
\newlabel{recon_input}{{23}{12}{Network with spiking input}{equation.4.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces While the error convergence of the spiking-input and continous-input network matches, the error of the spiking-input network is increased by roughly a factor of 10, due to the loss of precision imposed by the spikes (note the green scale).}}{14}{figure.8}\protected@file@percent }
\newlabel{fig:spiking_vs_continous}{{8}{14}{While the error convergence of the spiking-input and continous-input network matches, the error of the spiking-input network is increased by roughly a factor of 10, due to the loss of precision imposed by the spikes (note the green scale)}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}The weights on-chip}{14}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Caption here!}}{14}{figure.9}\protected@file@percent }
\newlabel{fig:spiking_vs_continous}{{9}{14}{Caption here!}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Batched updates}{15}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Caption here!}}{16}{figure.10}\protected@file@percent }
\newlabel{fig:batched_update}{{10}{16}{Caption here!}{figure.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Further alignment using time-window update}{17}{subsubsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}Final pseudo code}{18}{subsubsection.4.2.5}\protected@file@percent }
\newlabel{sec:pseudo-code}{{4.2.5}{18}{Final pseudo code}{subsubsection.4.2.5}{}}
\newlabel{algorithm_chip_in_the_loop}{{2}{18}{Final pseudo code}{algocfline.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces The final pseudo code illustrating the changes made, compared to algorithm \ref  {algorithm_simulation}. Note for example the batched updates or the discretized feed-forward matrix in the beginning. All hyperparameters used can be found in the Appendix. }}{18}{algocf.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Results}{19}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Please reproduce figure with correct size!}}{19}{figure.11}\protected@file@percent }
\newlabel{fig:DYNAPS_reconstruction}{{11}{19}{Please reproduce figure with correct size!}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Please reproduce figure with correct size!}}{20}{figure.12}\protected@file@percent }
\newlabel{fig:DYNAPS_convergence}{{12}{20}{Please reproduce figure with correct size!}{figure.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{20}{section.5}\protected@file@percent }
\bibdata{bibliography}
\bibcite{chow:68}{{1}{1968}{{Chow and Liu}}{{}}}
