
@misc{xu2019learning,
    title={Learning an Adaptive Learning Rate Schedule},
    author={Zhen Xu and Andrew M. Dai and Jonas Kemp and Luke Metz},
    year={2019},
    eprint={1909.09712},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@Article{Sutton1988,
author="Sutton, Richard S.",
title="Learning to predict by the methods of temporal differences",
journal="Machine Learning",
year="1988",
month="Aug",
day="01",
volume="3",
number="1",
pages="9--44",
abstract="This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.",
issn="1573-0565",
doi="10.1007/BF00115009",
url="https://doi.org/10.1007/BF00115009"
}

@misc{wiel2017learning,
    title={Learning to represent signals spike by spike},
    author={Wieland Brendel and Ralph Bourdoukan and Pietro Vertechi and Christian K. Machens and Sophie Denéve},
    year={2017},
    eprint={1703.03777},
    archivePrefix={arXiv},
    primaryClass={q-bio.NC}
}

@article{Maass:2002:RCW:639717.639718,
 author = {Maass, Wolfgang and Natschlager, Thomas and Markram, Henry},
 title = {Real-time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations},
 journal = {Neural Comput.},
 issue_date = {November 2002},
 volume = {14},
 number = {11},
 month = nov,
 year = {2002},
 issn = {0899-7667},
 pages = {2531--2560},
 numpages = {30},
 url = {http://dx.doi.org/10.1162/089976602760407955},
 doi = {10.1162/089976602760407955},
 acmid = {639718},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@article {Hopfield2554,
	author = {Hopfield, J J},
	title = {Neural networks and physical systems with emergent collective computational abilities},
	volume = {79},
	number = {8},
	pages = {2554--2558},
	year = {1982},
	doi = {10.1073/pnas.79.8.2554},
	publisher = {National Academy of Sciences},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/79/8/2554},
	eprint = {https://www.pnas.org/content/79/8/2554.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@inproceedings{Bourdoukan:2012:LOS:2999325.2999390,
 author = {Bourdoukan, Ralph and Barrett, David G. T. and Machens, Christian K. and Den\`{e}ve, Sophie},
 title = {Learning Optimal Spike-based Representations},
 booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'12},
 year = {2012},
 location = {Lake Tahoe, Nevada},
 pages = {2285--2293},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2999325.2999390},
 acmid = {2999390},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@Book{Lapicque,
  title = {Recherches quantitatives sur l'excitation électrique des nerfs traitée comme une polarisation},
  author = {Louis Lapicque},
  year = {1907},
  url = {https://fr.wikisource.org/wiki/Recherches_quantitatives_sur_l%27excitation_%C3%A9lectrique_des_nerfs_trait%C3%A9e_comme_une_polarisation},
}

@Article{Dale1935,
author={Dale, H.},
title={Pharmacology and Nerve-endings (Walter Ernest Dixon Memorial Lecture): (Section of Therapeutics and Pharmacology)},
journal={Proceedings of the Royal Society of Medicine},
year={1935},
month={Jan},
volume={28},
number={3},
pages={319-332},
abstract={A brief account is given of the scientific career of Walter Ernest Dixon, and of the importance of his work and his influence for the development of Pharmacology in England. It is suggested that the Memorial Lecture may appropriately deal with some matter of new interest, from one of the fields of research in which Dixon himself was active. Special mention is made of his work with Brodie on the physiology and pharmacology of the bronchioles and the pulmonary blood-vessels, as probably showing the beginning of Dixon's interest in the actions of the alkaloids and organic bases which reproduce the effects of autonomic nerves.An account is given of Dixon's early interest in the suggestion, first made by Elliott, that autonomic nerves transmit their effects by releasing, at their endings, specific substances, which reproduce their actions; and of his attempt to obtain experimental support for this conception. After the War it was established by the experiments of O. Loewi; and it is now generally recognized that parasympathetic effects are so transmitted by release of acetylcholine, sympathetic effects by that of a substance related to adrenaline.Very recent evidence indicates that acetylcholine, by virtue of its other ("nicotine-like") action, also acts as transmitter of activity at synapses in autonomic ganglia, and from motor nerve to voluntary muscle.The terms "cholinergic" and "adrenergic" have been introduced to describe nerve-fibres which transmit their actions by the release at their endings of acetylcholine, and of a substance related to adrenaline, respectively. It is shown that Langley and Anderson's evidence, long available, as to the kinds of peripheral efferent fibres which can replace one another in regeneration, can be summarized by the statement, that cholinergic can replace cholinergic fibres, and that adrenergic can replace adrenergic fibres; but that fibres of different chemical function cannot replace one another. The bearing of this new evidence on conceptions of the mode of action of "neuromimetic" drugs is discussed. The pharmacological problem can now be more clearly defined, and Dixon's participation in further attempts at its solution will be sadly missed.},
note={19990108[pmid]},
note={PMC2205701[pmcid]},
issn={0035-9157},
url={https://www.ncbi.nlm.nih.gov/pubmed/19990108}
}

@Article{Tibshirani1996,
author={Tibshirani, Robert},
title={Regression Shrinkage and Selection via the Lasso},
year={1996},
publisher={[Royal Statistical Society, Wiley]},
volume={58},
number={1},
pages={267-288},
abstract={[We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.]},
issn={00359246},
url={www.jstor.org/stable/2346178}
}

@inproceedings{Vincent:2008:ECR:1390156.1390294,
 author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
 title = {Extracting and Composing Robust Features with Denoising Autoencoders},
 booktitle = {Proceedings of the 25th International Conference on Machine Learning},
 series = {ICML '08},
 year = {2008},
 isbn = {978-1-60558-205-4},
 location = {Helsinki, Finland},
 pages = {1096--1103},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1390156.1390294},
 doi = {10.1145/1390156.1390294},
 acmid = {1390294},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{DBLP:journals/corr/abs-1708-04198,
  author    = {Saber Moradi and
               Qiao Ning and
               Fabio Stefanini and
               Giacomo Indiveri},
  title     = {A scalable multi-core architecture with heterogeneous memory structures
               for Dynamic Neuromorphic Asynchronous Processors (DYNAPs)},
  journal   = {CoRR},
  volume    = {abs/1708.04198},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.04198},
  archivePrefix = {arXiv},
  eprint    = {1708.04198},
  timestamp = {Mon, 13 Aug 2018 16:46:31 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-04198},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{Deneve2016,
author={Den{\`e}ve, Sophie
and Machens, Christian K.},
title={Efficient codes and balanced networks},
journal={Nature Neuroscience},
year={2016},
volume={19},
number={3},
pages={375-382},
abstract={Despite representing a minority of cortical cells, inhibitory neurons deeply shape cortical responses. Inhibitory currents closely track excitatory currents, opening only brief windows of opportunity for a neuron to fire. This explains the variability of cortical spike trains, but may also, paradoxically, render a spiking network maximally efficient and precise.},
issn={1546-1726},
doi={10.1038/nn.4243},
url={https://doi.org/10.1038/nn.4243}
}


@Article{Boybat2018,
author={Boybat, Irem
and Le Gallo, Manuel
and Nandakumar, S. R.
and Moraitis, Timoleon
and Parnell, Thomas
and Tuma, Tomas
and Rajendran, Bipin
and Leblebici, Yusuf
and Sebastian, Abu
and Eleftheriou, Evangelos},
title={Neuromorphic computing with multi-memristive synapses},
journal={Nature Communications},
year={2018},
volume={9},
number={1},
pages={2514},
abstract={Neuromorphic computing has emerged as a promising avenue towards building the next generation of intelligent computing systems. It has been proposed that memristive devices, which exhibit history-dependent conductivity modulation, could efficiently represent the synaptic weights in artificial neural networks. However, precise modulation of the device conductance over a wide dynamic range, necessary to maintain high network accuracy, is proving to be challenging. To address this, we present a multi-memristive synaptic architecture with an efficient global counter-based arbitration scheme. We focus on phase change memory devices, develop a comprehensive model and demonstrate via simulations the effectiveness of the concept for both spiking and non-spiking neural networks. Moreover, we present experimental results involving over a million phase change memory devices for unsupervised learning of temporal correlations using a spiking neural network. The work presents a significant step towards the realization of large-scale and energy-efficient neuromorphic computing systems.},
issn={2041-1723},
doi={10.1038/s41467-018-04933-y},
url={https://doi.org/10.1038/s41467-018-04933-y}
}

@ARTICLE{corradi, 
author={F. Corradi and G. Indiveri}, 
journal={IEEE Transactions on Biomedical Circuits and Systems}, 
title={A Neuromorphic Event-Based Neural Recording System for Smart Brain-Machine-Interfaces}, 
year={2015}, 
volume={9}, 
number={5}, 
pages={699-709}, 
keywords={analogue-digital conversion;band-pass filters;biomedical electronics;brain-computer interfaces;data compression;low noise amplifiers;low-power electronics;medical signal processing;neurophysiology;pattern recognition;neuromorphic event-based neural recording system;smart brain-machine-interfaces;neuromorphic spiking neural processing circuits;data compression;signal processing;neural computation;low-noise amplifier;delta-modulator analog-to-digital converter;low-power band-pass filter;bioamplifier;delta-modulator circuits;event-based communication protocols;pattern recognition;Neuromorphics;Neurons;Detectors;Capacitors;Biological neural networks;Noise;Transistors;Neural recordings;neuromorphic system;very large scale integration (VLSI);Acoustic Stimulation;Amplifiers, Electronic;Animals;Biomedical Engineering;Birds;Brain;Brain-Computer Interfaces;Equipment Design;Neurons;Neurosciences;Signal Processing, Computer-Assisted}, 
doi={10.1109/TBCAS.2015.2479256}, 
ISSN={1940-9990}, 
month={Oct},}

@inproceedings{Cassidy2016TrueNorthAH,
  title={TrueNorth: A High-Performance, Low-Power Neurosynaptic Processor for Multi-Sensory Perception, Action, and Cognition},
  author={Andrew S. Cassidy and Jun Sawada and Paul Merolla and John V. Arthur and Rodrigo Alvarez-Icaza and Filipp Akopyan and Bryan L. Jackson and Dharmendra S. Modha},
  year={2016}
}

@incollection{Deiss:1999:PCI:296533.296540,
 author = {Deiss, Stephen R. and Douglas, Rodney J. and Whatley, Adrian M.},
 chapter = {A Pulse-coded Communications Infrastructure for Neuromorphic Systems},
 title = {Pulsed Neural Networks},
 editor = {Maass, Wolfgang and Bishop, Christopher M.},
 year = {1999},
 isbn = {0-626-13350-4},
 pages = {157--178},
 numpages = {22},
 url = {http://dl.acm.org/citation.cfm?id=296533.296540},
 acmid = {296540},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 